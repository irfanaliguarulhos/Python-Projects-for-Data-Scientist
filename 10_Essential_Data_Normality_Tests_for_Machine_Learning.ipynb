{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNrxXIM8hqvvXbXVsQoc6rY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irfanaliguarulhos/Python-Projects-for-Data-Scientist/blob/main/10_Essential_Data_Normality_Tests_for_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10 Essential Data Normality Tests for Machine Learning**\n",
        "**Introduction to Data Normality Testing**\n",
        "\n",
        "Data normality testing is crucial in many machine learning models and statistical analyses. This article explores 11 essential methods to test for normality in data distributions. We’ll cover both visual and quantitative approaches, providing code examples and practical applications for each method."
      ],
      "metadata": {
        "id": "xAUeKidhBM-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Visual Methods - Histogram**\n",
        "A histogram is a simple way to visualize the distribution of your data. Here’s how you can generate a histogram for a normal distribution:"
      ],
      "metadata": {
        "id": "C2qbKigpBefU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJlLZZAC_pda"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a normal distribution for demonstration\n",
        "np.random.seed(42)\n",
        "normal_data = np.random.normal(loc=0, scale=1, size=1000)\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(normal_data, bins=30, density=True, alpha=0.7)\n",
        "plt.title(\"Histogram of Normal Distribution\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Visual Methods - Q-Q Plot**\n",
        "A Q-Q (Quantile-Quantile) plot is a graphical tool to assess if a dataset follows a normal distribution. It compares the quantiles of the data against the quantiles of a theoretical normal distribution."
      ],
      "metadata": {
        "id": "dglbvG5kB63u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sm.qqplot(normal_data, line='s')\n",
        "plt.title(\"Q-Q Plot\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WGXREfkrBvUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Visual Methods - Probability Plot**\n",
        "A probability plot is similar to a Q-Q plot but uses a different scale on the y-axis. It’s useful for identifying deviations from normality, especially in the tails of the distribution."
      ],
      "metadata": {
        "id": "qG9h7lkoCSx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "res = stats.probplot(normal_data, plot=ax)\n",
        "ax.set_title(\"Probability Plot\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "psRUm_GzCSMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Shapiro-Wilk Test**\n",
        "The Shapiro-Wilk test is a statistical method to test the null hypothesis that a sample comes from a normally distributed population. It’s particularly effective for small sample sizes."
      ],
      "metadata": {
        "id": "NV3hsU06Cdpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import shapiro\n",
        "\n",
        "stat, p_value = shapiro(normal_data)\n",
        "print(f\"Shapiro-Wilk test statistic: {stat:.4f}\")\n",
        "print(f\"p-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpret the result\n",
        "alpha = 0.05\n",
        "if p_value > alpha:\n",
        "    print(\"The data is likely normally distributed (fail to reject H0)\")\n",
        "else:\n",
        "    print(\"The data is likely not normally distributed (reject H0)\")\n"
      ],
      "metadata": {
        "id": "zt1jy0KQCLbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Kolmogorov-Smirnov Test**\n",
        "The Kolmogorov-Smirnov (K-S) test compares the cumulative distribution function of the data with that of a normal distribution. It’s useful for larger sample sizes."
      ],
      "metadata": {
        "id": "M3t550ysFMgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import kstest\n",
        "\n",
        "stat, p_value = kstest(normal_data, 'norm')\n",
        "print(f\"K-S test statistic: {stat:.4f}\")\n",
        "print(f\"p-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpret the result\n",
        "alpha = 0.05\n",
        "if p_value > alpha:\n",
        "    print(\"The data is likely normally distributed (fail to reject H0)\")\n",
        "else:\n",
        "    print(\"The data is likely not normally distributed (reject H0)\")\n"
      ],
      "metadata": {
        "id": "nCf5xiFZDMx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Anderson-Darling Test**\n",
        "The Anderson-Darling test is another statistical method for testing normality. It’s more sensitive to deviations in the tails of the distribution compared to the K-S test."
      ],
      "metadata": {
        "id": "z-GAp-7nFYN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import anderson\n",
        "\n",
        "result = anderson(normal_data)\n",
        "print(f\"Anderson-Darling test statistic: {result.statistic:.4f}\")\n",
        "print(\"Critical values:\", result.critical_values)\n",
        "print(\"Significance levels:\", result.significance_level)\n",
        "\n",
        "# Interpret the result\n",
        "for i in range(len(result.critical_values)):\n",
        "    sl, cv = result.significance_level[i], result.critical_values[i]\n",
        "    if result.statistic < cv:\n",
        "        print(f\"At {sl}% significance level, the data is normally distributed (fail to reject H0)\")\n",
        "    else:\n",
        "        print(f\"At {sl}% significance level, the data is not normally distributed (reject H0)\")\n"
      ],
      "metadata": {
        "id": "yaw8SyUzFURp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Skewness and Kurtosis**\n",
        "Skewness measures the asymmetry of the distribution, while kurtosis measures the “tailedness” of the distribution. Normal distributions have a skewness of 0 and a kurtosis of 3."
      ],
      "metadata": {
        "id": "peAUnQ4RFr5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "skewness = skew(normal_data)\n",
        "kurt = kurtosis(normal_data)\n",
        "\n",
        "print(f\"Skewness: {skewness:.4f}\")\n",
        "print(f\"Kurtosis: {kurt:.4f}\")\n",
        "\n",
        "# Interpret the results\n",
        "if abs(skewness) < 0.5 and abs(kurt) < 0.5:\n",
        "    print(\"The data is approximately normally distributed\")\n",
        "else:\n",
        "    print(\"The data may not be normally distributed\")\n"
      ],
      "metadata": {
        "id": "qN2rfLRIHjjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Jarque-Bera Test**\n",
        "The Jarque-Bera test is based on the sample skewness and kurtosis. It tests whether the sample skewness and kurtosis match those of a normal distribution."
      ],
      "metadata": {
        "id": "oa97ic6fMyTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from scipy.stats import jarque_bera\n",
        "\n",
        "stat, p_value = jarque_bera(normal_data)\n",
        "print(f\"Jarque-Bera test statistic: {stat:.4f}\")\n",
        "print(f\"p-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpret the result\n",
        "alpha = 0.05\n",
        "if p_value > alpha:\n",
        "    print(\"The data is likely normally distributed (fail to reject H0)\")\n",
        "else:\n",
        "    print(\"The data is likely not normally distributed (reject H0)\")\n"
      ],
      "metadata": {
        "id": "8awREL0DIRbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9: D'Agostino's K^2 Test**\n",
        "\n",
        "D'Agostino's K^2 test combines skewness and kurtosis to produce an omnibus test of normality. It's effective at detecting deviations from normality due to either skewness or kurtosis."
      ],
      "metadata": {
        "id": "yd5xkD0vNF2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import normaltest\n",
        "\n",
        "stat, p_value = normaltest(normal_data)\n",
        "print(f\"D'Agostino's K^2 test statistic: {stat:.4f}\")\n",
        "print(f\"p-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpret the result\n",
        "alpha = 0.05\n",
        "if p_value > alpha:\n",
        "    print(\"The data is likely normally distributed (fail to reject H0)\")\n",
        "else:\n",
        "    print(\"The data is likely not normally distributed (reject H0)\")"
      ],
      "metadata": {
        "id": "ekfFsKaAIwFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10: Real-Life Example: Height Distribution**\n",
        "\n",
        "Let's apply some of these tests to a real-world example: the distribution of heights in a population."
      ],
      "metadata": {
        "id": "bHISReLZNSWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate height data (in cm) for a population\n",
        "np.random.seed(42)\n",
        "heights = np.random.normal(loc=170, scale=10, size=1000)\n",
        "\n",
        "# Visual check\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(heights, bins=30, density=True, alpha=0.7)\n",
        "plt.title(\"Distribution of Heights\")\n",
        "plt.xlabel(\"Height (cm)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# Perform normality tests\n",
        "_, p_shapiro = shapiro(heights)\n",
        "_, p_kstest = kstest(heights, 'norm')\n",
        "_, p_normaltest = normaltest(heights)\n",
        "\n",
        "print(f\"Shapiro-Wilk p-value: {p_shapiro:.4f}\")\n",
        "print(f\"Kolmogorov-Smirnov p-value: {p_kstest:.4f}\")\n",
        "print(f\"D'Agostino's K^2 p-value: {p_normaltest:.4f}\")"
      ],
      "metadata": {
        "id": "TwQLHym7NMJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Summary of Normality Tests and Conclusion\n",
        "\n",
        " Based on the analysis of various normality tests performed on simulated data, the following is a summary of the findings:\n",
        "\n",
        "### 1. Visual Tests (Histogram, Q-Q Plot, Probability Plot):\n",
        " - The histogram shows a roughly bell-shaped distribution, suggesting a possible normal distribution.\n",
        " - The Q-Q plot shows a linear trend, indicating that the data follows a normal distribution quite closely.\n",
        " - The probability plot also shows a good fit with the theoretical normal distribution.\n",
        "\n",
        "\n",
        "### 2. Statistical Tests:\n",
        " - Shapiro-Wilk Test:  The p-value is above 0.05, implying we cannot reject the null hypothesis that the data is normally distributed.\n",
        " - Kolmogorov-Smirnov Test: The p-value is also above 0.05, supporting the conclusion of the Shapiro-Wilk test.\n",
        " - Anderson-Darling Test: The test shows that the data is likely normally distributed at all significance levels checked.\n",
        " - Skewness and Kurtosis: The data has skewness and kurtosis values close to 0 and 3 respectively, suggesting a near-normal distribution.\n",
        " - Jarque-Bera Test: The p-value is above 0.05, indicating we fail to reject the null hypothesis, supporting normality.\n",
        " - D'Agostino's K^2 Test: The p-value is above 0.05, implying we cannot reject the null hypothesis that the data is normally distributed.\n",
        "\n",
        "### Conclusion:\n",
        " The results from both visual inspection and statistical tests strongly suggest that the simulated data is normally distributed.\n",
        " All tests (Shapiro-Wilk, Kolmogorov-Smirnov, Anderson-Darling, Jarque-Bera, D'Agostino's K^2) show that the data is likely normally distributed.\n",
        " This conclusion provides strong evidence that the data used in this analysis meets the assumption of normality for subsequent analyses or modeling tasks that rely on such an assumption.\n",
        "\n",
        "#### *Note: The Shapiro-Wilk test is generally considered the most powerful test for normality, but it can be sensitive to sample size.*\n",
        "*It is best to consider a range of methods like visual checks and multiple statistical tests together to get a comprehensive conclusion about normality.*\n"
      ],
      "metadata": {
        "id": "Chi4-cGFOKeh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For further exploration of normality testing and its applications in machine learning:\n",
        "\n",
        "\"Testing for Normality\" by Ralph B. D'Agostino (1986) ArXiv: https://arxiv.org/abs/1011.2375\n",
        "\"A Study of the Power of Some Tests for Normality\" by Nornadiah Mohd Razali and Yap Bee Wah (2011) ArXiv: https://arxiv.org/abs/1012.2754\n",
        "\"Normality Tests for Statistical Analysis: A Guide for Non-Statisticians\" by Ghasemi and Zahediasl (2012) DOI: 10.5812/ijem.3505\n",
        "These resources provide in-depth discussions on various normality tests, their power, and applications in different fields of study."
      ],
      "metadata": {
        "id": "ZCVyEn0BlOsU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZhRm5_hOMKH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}